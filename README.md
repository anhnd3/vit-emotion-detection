# Emotion Recognition with Vision Transformer (ViT)

This project is all about recognizing human emotions using imagesâ€”specifically with the help of Googleâ€™s Vision Transformer (ViT). It's a modern take on emotion detection, using transformer-based models that analyze images by breaking them into patches, just like words in a sentence.

## What This Project Does

Understanding emotions is key for making smarter, more human-friendly tech. Whether itâ€™s healthcare, marketing, or in your car, detecting how people feel can improve the experience. Weâ€™ve built a system that uses ViT to do exactly thatâ€”spot emotions in faces with high accuracy.

### ğŸ“Œ How It's Setup

Hereâ€™s a quick look at how everythingâ€™s organized:

``` plain/text
â”œâ”€â”€ Application
â”‚   â”œâ”€â”€ emotion-detection-ui      # Front-end application built with React & rspack
â”‚   â””â”€â”€ emotion-services          # Backend services built with FastAPI & PyTorch (ViT model)
â”œâ”€â”€ Training                      # Fine-tuning/Training code with FER-2013 dataset here
â””â”€â”€ README.md                     # Main documentation for overall project overview and quick start
```

## ğŸš€ Getting Started

Hereâ€™s how to get the project up and running:

### 1. Setup Client (Front-end)

Head over to the [Frontend README](./Application/emotion-detection-ui/README.md) for setup instructions.

### 2. Setup Server (Back-end Services)

Check the [Backend README](./Application/emotion-services/README.md) for how to start the backend services.


## ğŸ–¼ï¸ What It Looks Like

Here are a few snapshots of the system in action:

![Result 01](./images/result_01.png)

![Result 02](./images/result_02.png)

![Result 03](./images/result_03.png)

![Result 04](./images/result_04.png)

## ğŸ¯ What We Built â€” and Why It Matters

We built a real-time **emotion recognition MVP** powered by video streaming and Google's Vision Transformer (ViT).
ğŸ¥ [Watch the video demo](https://drive.google.com/file/d/1ejYeT68z1jnLx-gyZPks0DS0VcFzzQpa/view?usp=drive_link)

### ğŸ”§ How It Works

- The **frontend** (React) streams live video from the user's webcam.
- The **backend** (FastAPI + PyTorch) receives video frames, runs them through a ViT model, and returns emotion predictions.
- The system detects emotions like happiness, sadness, surprise, and more â€” frame by frame â€” and displays results instantly in the UI.

Itâ€™s fast, lightweight, and works locally or on a server. Perfect as a foundation for smarter, emotion-aware applications.

---

But this kind of tech isnâ€™t just cool â€” itâ€™s already making an impact in the real world.

### ğŸŒ Real-World Use Cases

**Advertising**  
*Kelloggâ€™s Emotion Analytics*  
Kelloggâ€™s uses emotion recognition to test different ad versions. Viewersâ€™ facial reactions are analyzed to see which ads grab attention â€” helping marketers improve engagement.

**Healthcare**  
*Monitoring Non-Communicative Patients*  
Hospitals use emotion AI to track the emotional state of patients who canâ€™t speak or show clear expressions, like those with epilepsy or after a stroke â€” aiding diagnosis and care.

**Automotive**  
*Driver Monitoring by Eyeris*  
In-car emotion detection keeps tabs on the driverâ€™s mood and alertness. If someoneâ€™s drowsy or distracted, the system can raise an alert before something goes wrong.

**Sports & Entertainment**  
*Fan Experience in Stadiums*  
Stadiums use emotion tracking to read crowd mood and adjust entertainment in real time â€” boosting engagement and atmosphere.

**Personal Safety**  
*Epowar App + Smartwatches*  
Epowar combines wearables and AI to detect physical distress (like a spike in heart rate or sudden movement). It can trigger alerts and record events for personal safety.

---

From our MVP to global applications, emotion recognition is already changing how we interact with technology â€” and each other.


## ğŸ“– References & Resources

- [Vision Transformer Paper (ViT)](https://arxiv.org/abs/2010.11929)
- [GitHub Repository - ViT Official](https://github.com/google-research/vision_transformer)

## ğŸ‘¥ Team & Contact

- Group 10
  - Nguyá»…n Duy Anh - 24C15002 - <24C15002@student.hcmus.edu.vn>
  - Nguyá»…n Pháº¡m KhÃ´i NguyÃªn - 24C15013 - <24C15013@student.hcmus.edu.vn>
  - Nguyá»…n Trá»ng PhÃºc - 24C15019 - <24C15019@student.hcmus.edu.vn>
  - Há»“ng Nháº¥t PhÆ°Æ¡ng - 24C15051 - <24C15051@student.hcmus.edu.vn>
